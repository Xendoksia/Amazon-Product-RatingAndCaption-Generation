import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, models
import pandas as pd
from PIL import Image
import os
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import json
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

class ProductImageDataset(Dataset):
    """Dataset class for product images and ratings"""
    
    def __init__(self, metadata_csv, images_dir, transform=None):
        self.metadata = pd.read_csv(metadata_csv)
        self.images_dir = images_dir
        self.transform = transform
        
        # Remove any rows with missing ratings
        self.metadata = self.metadata.dropna(subset=['rating'])
        
        # Filter out rows where image files don't exist
        before_count = len(self.metadata)
        existing_files = []
        
        for idx, row in self.metadata.iterrows():
            image_path = os.path.join(self.images_dir, row['filename'])
            if os.path.exists(image_path):
                existing_files.append(idx)
        
        self.metadata = self.metadata.loc[existing_files].reset_index(drop=True)
        after_count = len(self.metadata)
        
        if before_count != after_count:
            print(f"Filtered dataset: {before_count} -> {after_count} samples (removed {before_count - after_count} missing files)")
        else:
            print(f"Dataset initialized with {after_count} samples")
    
    
    def __len__(self):
        return len(self.metadata)
    
    def __getitem__(self, idx):
        row = self.metadata.iloc[idx]
        
        # Load image
        image_path = os.path.join(self.images_dir, row['filename'])
        
        try:
            image = Image.open(image_path).convert('RGB')
        except Exception as e:
            # Return a black image if file is corrupted
            print(f"Warning: Could not load image {image_path}: {e}")
            image = Image.new('RGB', (224, 224), color='black')
        
        # Apply transforms
        if self.transform:
            image = self.transform(image)
        
        # Get rating (ensure float32 type)
        rating = torch.tensor(float(row['rating']), dtype=torch.float32)
        
        return image, rating, row['product_id']

class RatingPredictionModel(nn.Module):
    """Neural network model for rating prediction"""
    
    def __init__(self, backbone='resnet50', pretrained=True, dropout=0.5):
        super(RatingPredictionModel, self).__init__()
        
        self.backbone_name = backbone
        
        # Load backbone
        if backbone == 'resnet50':
            self.backbone = models.resnet50(pretrained=pretrained)
            num_features = self.backbone.fc.in_features
            self.backbone.fc = nn.Identity()  # Remove final layer
        elif backbone == 'resnet18':
            self.backbone = models.resnet18(pretrained=pretrained)
            num_features = self.backbone.fc.in_features
            self.backbone.fc = nn.Identity()
        elif backbone == 'efficientnet':
            self.backbone = models.efficientnet_b0(pretrained=pretrained)
            num_features = self.backbone.classifier[1].in_features
            self.backbone.classifier = nn.Identity()
        else:
            raise ValueError(f"Unsupported backbone: {backbone}")
        
        # Custom classifier head
        self.classifier = nn.Sequential(
            nn.Dropout(dropout),
            nn.Linear(num_features, 512),
            nn.ReLU(),
            nn.Dropout(dropout * 0.7),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(dropout * 0.5),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, 1)
        )
    
    def forward(self, x):
        features = self.backbone(x)
        output = self.classifier(features)
        return output.squeeze()

class RatingPredictor:
    """Main class for training and evaluating rating prediction models"""
    
    def __init__(self, dataset_dir='product_dataset', model_save_dir='models'):
        self.dataset_dir = dataset_dir
        self.model_save_dir = model_save_dir
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # Create model save directory
        os.makedirs(model_save_dir, exist_ok=True)
        
        print(f"Using device: {self.device}")
    
    def get_transforms(self, augment=True):
        """Get data transforms for training and validation"""
        if augment:
            # Training transforms with augmentation
            train_transform = transforms.Compose([
                transforms.Resize((256, 256)),
                transforms.RandomCrop(224),
                transforms.RandomHorizontalFlip(0.5),
                transforms.RandomRotation(degrees=15),
                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
                transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
            ])
        else:
            train_transform = transforms.Compose([
                transforms.Resize((224, 224)),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
            ])
        
        # Validation/test transforms (no augmentation)
        val_transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        
        return train_transform, val_transform
    
    def create_data_loaders(self, batch_size=16, num_workers=4, augment=True):
        """Create data loaders for training, validation, and testing"""
        train_transform, val_transform = self.get_transforms(augment)
        
        # Create datasets
        train_dataset = ProductImageDataset(
            os.path.join(self.dataset_dir, 'train_metadata.csv'),
            os.path.join(self.dataset_dir, 'images'),
            transform=train_transform
        )
        
        val_dataset = ProductImageDataset(
            os.path.join(self.dataset_dir, 'val_metadata.csv'),
            os.path.join(self.dataset_dir, 'images'),
            transform=val_transform
        )
        
        test_dataset = ProductImageDataset(
            os.path.join(self.dataset_dir, 'test_metadata.csv'),
            os.path.join(self.dataset_dir, 'images'),
            transform=val_transform
        )
        
        # Create data loaders
        train_loader = DataLoader(
            train_dataset, batch_size=batch_size, shuffle=True, 
            num_workers=num_workers, pin_memory=True
        )
        
        val_loader = DataLoader(
            val_dataset, batch_size=batch_size, shuffle=False, 
            num_workers=num_workers, pin_memory=True
        )
        
        test_loader = DataLoader(
            test_dataset, batch_size=batch_size, shuffle=False, 
            num_workers=num_workers, pin_memory=True
        )
        
        return train_loader, val_loader, test_loader, train_dataset, val_dataset, test_dataset
    
    def train_model(self, model, train_loader, val_loader, num_epochs=50, 
                   learning_rate=0.001, weight_decay=1e-4, patience=10):
        """Train the rating prediction model"""
        
        # Loss and optimizer
        criterion = nn.MSELoss()
        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='min', factor=0.5, patience=patience//2, verbose=True
        )
        
        # Training history
        history = {
            'train_loss': [],
            'val_loss': [],
            'train_mae': [],
            'val_mae': [],
            'learning_rate': []
        }
        
        best_val_loss = float('inf')
        epochs_without_improvement = 0
        
        model.to(self.device)
        
        for epoch in range(num_epochs):
            # Training phase
            model.train()
            train_loss = 0.0
            train_predictions = []
            train_targets = []
            
            train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Train]')
            for batch_idx, (images, ratings, _) in enumerate(train_pbar):
                images, ratings = images.to(self.device), ratings.to(self.device, dtype=torch.float32)
                
                optimizer.zero_grad()
                outputs = model(images)
                loss = criterion(outputs, ratings)
                loss.backward()
                optimizer.step()
                
                train_loss += loss.item()
                train_predictions.extend(outputs.detach().cpu().numpy())
                train_targets.extend(ratings.detach().cpu().numpy())
                
                # Update progress bar
                train_pbar.set_postfix({
                    'loss': f'{loss.item():.4f}',
                    'avg_loss': f'{train_loss/(batch_idx+1):.4f}'
                })
            
            # Validation phase
            model.eval()
            val_loss = 0.0
            val_predictions = []
            val_targets = []
            
            with torch.no_grad():
                val_pbar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Val]')
                for batch_idx, (images, ratings, _) in enumerate(val_pbar):
                    images, ratings = images.to(self.device), ratings.to(self.device, dtype=torch.float32)
                    
                    outputs = model(images)
                    loss = criterion(outputs, ratings)
                    
                    val_loss += loss.item()
                    val_predictions.extend(outputs.cpu().numpy())
                    val_targets.extend(ratings.cpu().numpy())
                    
                    val_pbar.set_postfix({
                        'loss': f'{loss.item():.4f}',
                        'avg_loss': f'{val_loss/(batch_idx+1):.4f}'
                    })
            
            # Calculate metrics
            train_loss /= len(train_loader)
            val_loss /= len(val_loader)
            train_mae = mean_absolute_error(train_targets, train_predictions)
            val_mae = mean_absolute_error(val_targets, val_predictions)
            
            # Update history
            history['train_loss'].append(train_loss)
            history['val_loss'].append(val_loss)
            history['train_mae'].append(train_mae)
            history['val_mae'].append(val_mae)
            history['learning_rate'].append(optimizer.param_groups[0]['lr'])
            
            # Learning rate scheduler
            scheduler.step(val_loss)
            
            # Early stopping
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                epochs_without_improvement = 0
                # Save best model
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'val_loss': val_loss,
                    'history': history
                }, os.path.join(self.model_save_dir, 'best_model.pth'))
            else:
                epochs_without_improvement += 1
            
            print(f'Epoch {epoch+1}/{num_epochs}:')
            print(f'  Train Loss: {train_loss:.4f}, Train MAE: {train_mae:.4f}')
            print(f'  Val Loss: {val_loss:.4f}, Val MAE: {val_mae:.4f}')
            print(f'  Learning Rate: {optimizer.param_groups[0]["lr"]:.6f}')
            print(f'  Best Val Loss: {best_val_loss:.4f}')
            print('-' * 50)
            
            # Early stopping check
            if epochs_without_improvement >= patience:
                print(f'Early stopping after {epoch+1} epochs')
                break
        
        return history
    
    def evaluate_model(self, model, test_loader):
        """Evaluate model performance on test set"""
        model.eval()
        model.to(self.device)
        
        predictions = []
        targets = []
        product_ids = []
        
        with torch.no_grad():
            for images, ratings, p_ids in tqdm(test_loader, desc='Evaluating'):
                images, ratings = images.to(self.device), ratings.to(self.device, dtype=torch.float32)
                outputs = model(images)
                
                predictions.extend(outputs.cpu().numpy())
                targets.extend(ratings.cpu().numpy())
                product_ids.extend(p_ids.numpy())
        
        # Calculate metrics
        mse = mean_squared_error(targets, predictions)
        mae = mean_absolute_error(targets, predictions)
        rmse = np.sqrt(mse)
        r2 = r2_score(targets, predictions)
        
        # Clamp predictions to valid rating range (1-5)
        clamped_predictions = np.clip(predictions, 1.0, 5.0)
        clamped_mae = mean_absolute_error(targets, clamped_predictions)
        clamped_rmse = np.sqrt(mean_squared_error(targets, clamped_predictions))
        
        metrics = {
            'mse': mse,
            'mae': mae,
            'rmse': rmse,
            'r2': r2,
            'clamped_mae': clamped_mae,
            'clamped_rmse': clamped_rmse
        }
        
        print("Test Set Evaluation Results:")
        print(f"MSE: {mse:.4f}")
        print(f"MAE: {mae:.4f}")
        print(f"RMSE: {rmse:.4f}")
        print(f"R²: {r2:.4f}")
        print(f"Clamped MAE (1-5 range): {clamped_mae:.4f}")
        print(f"Clamped RMSE (1-5 range): {clamped_rmse:.4f}")
        
        return metrics, predictions, targets, product_ids
    
    def plot_training_history(self, history):
        """Plot training history"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # Loss plot
        axes[0, 0].plot(history['train_loss'], label='Train Loss')
        axes[0, 0].plot(history['val_loss'], label='Val Loss')
        axes[0, 0].set_title('Training and Validation Loss')
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('MSE Loss')
        axes[0, 0].legend()
        axes[0, 0].grid(True)
        
        # MAE plot
        axes[0, 1].plot(history['train_mae'], label='Train MAE')
        axes[0, 1].plot(history['val_mae'], label='Val MAE')
        axes[0, 1].set_title('Training and Validation MAE')
        axes[0, 1].set_xlabel('Epoch')
        axes[0, 1].set_ylabel('Mean Absolute Error')
        axes[0, 1].legend()
        axes[0, 1].grid(True)
        
        # Learning rate plot
        axes[1, 0].plot(history['learning_rate'])
        axes[1, 0].set_title('Learning Rate Schedule')
        axes[1, 0].set_xlabel('Epoch')
        axes[1, 0].set_ylabel('Learning Rate')
        axes[1, 0].set_yscale('log')
        axes[1, 0].grid(True)
        
        # Loss comparison
        axes[1, 1].plot(np.array(history['train_loss']) - np.array(history['val_loss']))
        axes[1, 1].set_title('Training vs Validation Loss Difference')
        axes[1, 1].set_xlabel('Epoch')
        axes[1, 1].set_ylabel('Train Loss - Val Loss')
        axes[1, 1].axhline(y=0, color='r', linestyle='--')
        axes[1, 1].grid(True)
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.model_save_dir, 'training_history.png'), dpi=300)
        plt.show()
    
    def plot_predictions(self, predictions, targets, save_name='test_predictions.png'):
        """Plot prediction vs actual ratings"""
        fig, axes = plt.subplots(1, 2, figsize=(15, 6))
        
        # Scatter plot
        axes[0].scatter(targets, predictions, alpha=0.6)
        axes[0].plot([1, 5], [1, 5], 'r--', label='Perfect Prediction')
        axes[0].set_xlabel('Actual Rating')
        axes[0].set_ylabel('Predicted Rating')
        axes[0].set_title('Predicted vs Actual Ratings')
        axes[0].legend()
        axes[0].grid(True)
        
        # Error histogram
        errors = np.array(predictions) - np.array(targets)
        axes[1].hist(errors, bins=30, alpha=0.7)
        axes[1].axvline(x=0, color='r', linestyle='--', label='No Error')
        axes[1].set_xlabel('Prediction Error')
        axes[1].set_ylabel('Frequency')
        axes[1].set_title('Distribution of Prediction Errors')
        axes[1].legend()
        axes[1].grid(True)
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.model_save_dir, save_name), dpi=300)
        plt.show()
    
    def train_and_evaluate(self, backbone='resnet50', batch_size=16, num_epochs=50, 
                          learning_rate=0.001, augment=True, dropout=0.5):
        """Complete training and evaluation pipeline"""
        print("🚀 Starting Rating Prediction Training Pipeline")
        print("=" * 60)
        print(f"📦 Model: {backbone}")
        print(f"🔧 Config: {batch_size} batch, {num_epochs} epochs, LR {learning_rate}")
        print(f"📊 Dropout: {dropout}, Augment: {augment}")
        print()
        
        # Create data loaders
        print("📁 Creating data loaders...")
        train_loader, val_loader, test_loader, train_dataset, val_dataset, test_dataset = \
            self.create_data_loaders(batch_size, augment=augment)
        
        # Create model
        print(f"🏗️ Creating {backbone} model...")
        model = RatingPredictionModel(backbone=backbone, pretrained=True, dropout=dropout)
        
        print(f"Model parameters: {sum(p.numel() for p in model.parameters()):,}")
        print(f"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}")
        
        # Train model
        print("Starting training...")
        history = self.train_model(model, train_loader, val_loader, 
                                 num_epochs=num_epochs, learning_rate=learning_rate)
        
        # Load best model for evaluation
        checkpoint = torch.load(os.path.join(self.model_save_dir, 'best_model.pth'))
        model.load_state_dict(checkpoint['model_state_dict'])
        
        # Evaluate on test set
        print("Evaluating on test set...")
        metrics, predictions, targets, product_ids = self.evaluate_model(model, test_loader)
        
        # Save results
        results = {
            'model_config': {
                'backbone': backbone,
                'batch_size': batch_size,
                'num_epochs': num_epochs,
                'learning_rate': learning_rate,
                'dropout': dropout,
                'augment': augment
            },
            'dataset_info': {
                'train_samples': len(train_dataset),
                'val_samples': len(val_dataset),
                'test_samples': len(test_dataset)
            },
            'metrics': metrics,
            'history': history
        }
        
        with open(os.path.join(self.model_save_dir, 'training_results.json'), 'w') as f:
            json.dump(results, f, indent=2, default=str)
        
        # Plot results
        self.plot_training_history(history)
        self.plot_predictions(predictions, targets)
        
        print("=" * 60)
        print("Training completed successfully!")
        print(f"Results saved to {self.model_save_dir}")
        
        return model, results

# ============================================================================
# CENTRALIZED CONFIGURATION - Tek yerden kontrol
# ============================================================================

# Model Configuration
MODEL_CONFIG = {
    'backbone': 'efficientnet',        # Modeller: resnet18, resnet50, efficientnet
    'batch_size': 16,                  
    'num_epochs': 30,                  
    'learning_rate': 0.005,            
    'dropout': 0.5,                   
    'augment': True,                   
}

# Dataset Configuration  
DATASET_CONFIG = {
    'dataset_dir': 'product_dataset',   # Veri dizini
    'model_save_dir': 'models'          # Model kayıt dizini
}

# Usage example
if __name__ == "__main__":
    print("🚀 Amazon Rating Prediction Training")
    print("=" * 50)
    print(f"📋 Configuration:")
    for key, value in MODEL_CONFIG.items():
        print(f"   {key}: {value}")
    print()
    
    # Initialize trainer
    trainer = RatingPredictor(
        dataset_dir=DATASET_CONFIG['dataset_dir'], 
        model_save_dir=DATASET_CONFIG['model_save_dir']
    )
    
    # Train and evaluate model
    model, results = trainer.train_and_evaluate(
        backbone=MODEL_CONFIG['backbone'],
        batch_size=MODEL_CONFIG['batch_size'],
        num_epochs=MODEL_CONFIG['num_epochs'],
        learning_rate=MODEL_CONFIG['learning_rate'],
        augment=MODEL_CONFIG['augment'],
        dropout=MODEL_CONFIG['dropout']
    )
    
    print(f"✅ Training completed! Results saved to {DATASET_CONFIG['model_save_dir']}")